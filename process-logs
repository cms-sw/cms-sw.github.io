#!/usr/bin/env python

# Generate a makefile to import all our AFS logs into the cms-sw.github.io
# repository so that we can happily serve them from there.
from __future__ import print_function
from glob import glob
from argparse import ArgumentParser
from os.path import join, exists, basename
from collections import defaultdict
from commands import getstatusoutput
import json
import re
from datetime import datetime, timedelta

RESULTS_PATH = "pyRelValMatrixLogs/run/runall-report-step123-.log"
UNITTEST_SUMMARY_PATH = "unitTests-summary.log"
RELVAL_LOGS = "pyRelValMatrixLogs.zip"

#
# makes a query to get information from elasticsearch
#
def query_es( query, es_index ):
  cmd = "curl -s -XPOST http://{hostname}/{index}/_search -d '{query}'".format(
         hostname=ES_HOSTNAME,
         index=es_index,
         query=query )
  err, out = getstatusoutput(cmd)
  if err:
    print( "Error while querying elasticsearch" )
    print( out )
    return {}

  try:
    response = json.loads( out )
  except Exception as e:
    print ('Error loading elasticsearch response')
    print( 'Reason: ' + str(e) )
    return {}

  return response


#
# Queries elasticsearch to get the current list of avaiable ib results for RelVals
#
def get_list_avaiable_ib_rv_results( es_hostname, es_index ):
  query_file = 'es-queries/list-available-ib-results.json'
  query = open( query_file, 'r' ).read()
  response = query_es( query, es_index )

  if not response.get( 'aggregations' ):
    print( "It seems that there is no information about the relvals for the IBs in elasticsearch.\n" \
           "I can continue, but this should be checked" )
    return []

  ibs = [ x[ 'key' ] for x in response[ 'aggregations' ][ 'IBs' ][ 'buckets' ] ]
  return ibs

#
# Queries elasticsearch to get the current list of avaiable ib results for SCRAM
#
def get_list_avaiable_ib_scram_results( es_hostname, es_index ):
  query_file = 'es-queries/list-available-scram-results.json'
  query = open( query_file, 'r' ).read()
  response = query_es( query, es_index )

  if not response.get( 'aggregations' ):
    print( "It seems that there is no information about the scram results of the IBs in elasticsearch.\n" \
           "I can continue, but this should be checked" )
    return []

  ibs = [ x[ 'key' ] for x in response[ 'aggregations' ][ 'IBs' ][ 'buckets' ] ]
  return ibs

#
# Removes form the list the IBs that are older than the date stated in the parameters.
# the date needs to be in the "IB" format. For example "2015-03-21-2300"
# returns a new list without the old IBs
#
def remove_too_old_ibs( earliest_date, ib_list ):
 
  return [ib for ib in ib_list if ( ib[ ib.rfind( '_' )+1:: ] >= earliest_date )]

if __name__ == "__main__":
  parser = ArgumentParser()
  parser.add_argument("--logdir", type=str, help="where to find the logs", required=True)
  parser.add_argument("--es_hostname", type=str, help="elasticsearch hostname", required=True)
  parser.add_argument("--es_rv_index", type=str, help="elasticsearch relvals index", required=True)
  parser.add_argument("--es_scram_index", type=str, help="elasticsearch scram index", required=True)
  parser.add_argument("--filter", type=str, default="")
  parser.add_argument("-n", "--dry-run", dest="dryRun", action="store_true", default=False)
  args = parser.parse_args()

  ES_HOSTNAME = args.es_hostname
  ES_RELVALS_INDEX = args.es_rv_index
  ES_SCRAM_INDEX = args.es_scram_index

  release_match = join(args.logdir, "([^/]*)/www/([^/]*)/([^/]*)/([^/]*)/pyRelValMatrixLogs/run/runall-report-step123-.log")

  # Generate a tuple with
  # (<release-name>, <release-queue>, <release-path>)
  globExpr = join(args.logdir, "*/www/*/*/*")
  releases = [r for r in glob(globExpr) if re.match(".*" + args.filter + ".*", r)]
  inputs = [join(r, RESULTS_PATH) for r in releases if exists(join(r, RESULTS_PATH))]
  inputsUnitTestSummary = [join(r, UNITTEST_SUMMARY_PATH) for r in releases if exists(join(r, UNITTEST_SUMMARY_PATH))]
  inputsRelvalLogs = [join(r, RELVAL_LOGS) for r in releases if exists(join(r, RELVAL_LOGS))]

  outputs = [re.match(release_match, r) for r in inputs]
  releases = [x.group(4) for x in outputs]
  dates = [re.match(".*(2[0-9]*-[01][0-9]-[0-3][0-9]-[0-9]*)$", x).group(1) for x in releases]
  queues = [re.match("(.*)_2[0-9]*-[01][0-9]-[0-3][0-9]-[0-9]*$", x).group(1) for x in releases]
  architectures = [m.group(1) for m in outputs]
  
  zip(architectures, dates, queues)
  outputs = [join("data/relvals/", x[0], x[1], x[2] + ".json")
             for x in zip(architectures, dates, queues)]
  outputsUnitTestSummary = [join("data/unitTests/", x[0], x[1], x[2] + ".json")
             for x in zip(architectures, dates, queues)]
  outputsRelvalCommands = [join("data/commands/", x[0], x[1], x[2] + ".json")
             for x in zip(architectures, dates, queues)]
  outputsRelvalMsgs = [join("data/messages/", x[0], x[1], x[2] + ".json")
             for x in zip(architectures, dates, queues)]
  outputsJobReports = [join("data/jobreports", x[0], x[2], x[1] + ".csv")
             for x in zip(architectures, dates, queues)]
  outputsInputFiles = [join("data/inputs", x[0], x[2], x[1] + ".csv")
             for x in zip(architectures, dates, queues)]
  pairs = zip(outputs, inputs)
  pairsUnitTestsSummary = zip(outputsUnitTestSummary, inputsUnitTestSummary)
  pairsCommands = zip(outputsRelvalCommands, inputsRelvalLogs)
  pairsMessages = zip(outputsRelvalMsgs, inputsRelvalLogs)
  pairsJobReports = zip(outputsJobReports, inputsRelvalLogs)
  pairsInputFiles = zip(outputsInputFiles, inputsRelvalLogs)

  #Incomplete results
  available_ibs_rv_es = get_list_avaiable_ib_rv_results( ES_HOSTNAME, ES_RELVALS_INDEX )
  available_ibs_scram_es = get_list_avaiable_ib_scram_results( ES_HOSTNAME, ES_SCRAM_INDEX )
  all_relvals_afs = [x.split('/')[ 10 ] for x in inputsRelvalLogs ]
  # for now, the list is obtained from the unit tests
  all_scram_res_afs = [x.split('/')[ 10 ] for x in inputsUnitTestSummary ] 

  incomplete_ibs_rv = list( set( available_ibs_rv_es ) - set( all_relvals_afs ) )
  dates.sort()
  incomplete_ibs_rv = remove_too_old_ibs( dates[0], incomplete_ibs_rv )
  incomplete_ibs_scram = list( set( available_ibs_scram_es ) - set( all_scram_res_afs ) )
  incomplete_ibs_scram = remove_too_old_ibs( dates[0], incomplete_ibs_scram )
  too_old_for_exceptions = datetime.now() - timedelta(days=2)
  ib_date_exception_limit = too_old_for_exceptions.strftime('%Y-%m-%d-%H00')
  ibs_to_check_exceptions = remove_too_old_ibs( ib_date_exception_limit, available_ibs_rv_es )

  print( 'All IBS with relvals results in ES:' )
  print( available_ibs_rv_es )
  print( 'All IBS with scram results in ES:' )
  print( available_ibs_scram_es )
  print( '--------------------' )
  print( 'Incomplete relvals results ( not in AFS ) older than %s:' % dates[0] )
  print( incomplete_ibs_rv )
  print( 'Incomplete scram results ( not in AFS ) older than %s:' % dates[0] )
  print( incomplete_ibs_scram )
  print( '------------' )
  print( 'Results in AFS:')
  print( all_relvals_afs ) 
  print( '---------' )
  print( 'Ibs for which I will check exceptions ( more recent than %s ):' % ib_date_exception_limit )
  print( ibs_to_check_exceptions )

  f = open("Makefile", "w")

  # A rule which takes care of converting a few known files in CMSDIST to json.

  allFiles = outputs + outputsUnitTestSummary + outputsRelvalCommands + outputsJobReports + outputsInputFiles
  print(".PHONY: all clean pr-stats", file=f)
  print("all: pr-stats ib-files data/cmsdist-config.json "+" ".join( ["incomplete_relvals_"+x for x in incomplete_ibs_rv] )+" "
         +" ".join( ["relvals_exceptions_"+x for x in available_ibs_rv_es] )+" "+" ".join( ["incomplete_scram_"+x for x in incomplete_ibs_scram] )+" " 
         +" ".join(allFiles)+" clean_up_old_files", file=f)
  print("data/cmsdist-config.json:\n\tmkdir -p `dirname $@` && ./process-cmsdist > $@", file=f)
  print("pr-stats:\n\tmkdir -p data/stats && ./process-pr-stats && cat data/stats/prs/*.csv | sort -u -r > data/stats/pr-stats.csv", file=f)
  print("ib-files:\n\tmkdir -p data/inputs && cat data/inputs/*/*/*.csv | sort -u -r > data/inputs/all-ibs.csv", file=f)
  print("clean:\n\trm -rf data/relvals", file=f)
  print("", file=f)
  for x in pairs:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./process-run123-logs $< > $@" % x
    print(l, file=f)
  for x in pairsUnitTestsSummary:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./process-unitTestSummary-logs $< > $@" % x
    print(l, file=f)
  for x in pairsCommands:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./process-step-commands $< data/commands/objs > $@" % x
    print(l, file=f)
  for x in pairsMessages:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./process-logs-messages $< data/messages/objs > $@" % x
    print(l, file=f)
  for x in pairsJobReports:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./process-job-reports $< > $@" % x
    print(l, file=f)
  for x in pairsInputFiles:
    l = "%s: %s\n\tmkdir -p `dirname $@` && ./log-input-files $< | sort > $@" % x
    print(l, file=f)
  for x in incomplete_ibs_rv:
    l = "incomplete_relvals_%s:\n\ttimeout 60 ./process-relvals-status-es %s --es_hostname %s --es_rv_index %s" % (x,x,ES_HOSTNAME,ES_RELVALS_INDEX)
    print(l, file=f)
  for x in available_ibs_rv_es:
    l = "relvals_exceptions_%s:\n\ttimeout 60 ./process-relvals-exceptions-es %s --es_hostname %s --es_rv_index %s" % (x,x,ES_HOSTNAME,ES_RELVALS_INDEX)
    print(l, file=f)
  for x in incomplete_ibs_scram:
    l = "incomplete_scram_%s:\n\ttimeout 60 ./process-scram-results-es %s --es_hostname %s --es_scram_index %s" % (x,x,ES_HOSTNAME,ES_SCRAM_INDEX)
    print(l, file=f)
  # print a rule for cleaning up the old files
  print( "clean_up_old_files:\n\t./cleanup-old-results -d %s" % dates[0], file=f )
